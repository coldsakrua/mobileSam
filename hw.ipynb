{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import monai\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.predictor import SamPredictor\n",
    "from segment_anything.modeling import ImageEncoderViT, MaskDecoder, PromptEncoder\n",
    "from segment_anything.automatic_mask_generator import SamAutomaticMaskGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device='cpu'\n",
    "model_path='./model/sam_vit_h_4b8939.pth'\n",
    "sam_model=sam_model_registry['vit_h'](checkpoint=model_path).to(device)\n",
    "predictor=SamPredictor(sam_model)\n",
    "\n",
    "num2color={\n",
    "    0:[0,0,0],\n",
    "    1:[0, 153, 255],\n",
    "    2:[102, 255, 153],\n",
    "    3:[0, 204, 153],\n",
    "    4:[255, 255, 102],\n",
    "    5:[255, 255, 204],\n",
    "    6:[255, 153, 0],\n",
    "    7:[255, 102, 255],\n",
    "    8:[102, 0, 51],\n",
    "    9:[255, 204, 255],\n",
    "    10:[255, 0, 102]\n",
    "}\n",
    "num2label={\n",
    "    0:'background',\n",
    "    1:'skin',\n",
    "    2:'left eyebrow',\n",
    "    3:'right eyebrow',\n",
    "    4:'left eye',\n",
    "    5:'right eye',\n",
    "    6:'nose',\n",
    "    7:'upper lip',\n",
    "    8:'inner mouth',\n",
    "    9:'lower lip',\n",
    "    10:'hair'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IG02DataSet(Dataset):\n",
    "    \n",
    "    def __init__(self,data_path,single=True):\n",
    "        self.data_path=data_path\n",
    "        self.data=sorted(glob(os.path.join(self.data_path,'*.image.png')))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        datapath=self.data[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaPaDataset(Dataset):\n",
    "    def __init__(self,data_path,label_path):\n",
    "        self.data_path=data_path\n",
    "        self.label_path=label_path\n",
    "\n",
    "        self.data=sorted(glob(os.path.join(self.data_path,'*.jpg')))\n",
    "        self.label=sorted(glob(os.path.join(self.label_path,'*.png')))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path=self.data[index]\n",
    "        label_path=self.label[index]\n",
    "        img=Image.open(data_path)\n",
    "        img=img.resize((1024,1024))\n",
    "        img=np.array(img)\n",
    "        \n",
    "        # img=torch.tensor(img,dtype=torch.float())\n",
    "        label=Image.open(label_path)\n",
    "        label=label.resize((1024,1024))\n",
    "        label=np.array(label)\n",
    "        label_list=[]\n",
    "        # label=torch.tensor(label)\n",
    "        center_list=[]\n",
    "        bbox_list=[]\n",
    "        embedding_list=[]\n",
    "        for i in range(1,11):\n",
    "            img=(img-img.min())/(img.max()-img.min())\n",
    "            img=img.transpose((2,0,1))\n",
    "            gt_mask=np.uint8(label==i)\n",
    "            # if (gt_mask.any()==i)==False:\n",
    "            #     center_list.append(None)\n",
    "            #     bbox_list.append(None)\n",
    "            #     continue\n",
    "            y_idx,x_idx=np.where(gt_mask>0)\n",
    "            y_min,y_max=np.min(y_idx),np.max(y_idx)\n",
    "            x_min,x_max=np.min(x_idx),np.max(x_idx)\n",
    "            dt_mask=cv2.distanceTransform(gt_mask[y_min:y_max+1,x_min:x_max+1],cv2.DIST_L2,3)\n",
    "            local_coords=np.unravel_index(np.argmax(dt_mask,),dt_mask.shape)\n",
    "            center_point=np.expand_dims(\n",
    "                np.array([local_coords[1],local_coords[0]])+np.array([x_min,y_min]),axis=0)\n",
    "            label_list.append(gt_mask)\n",
    "            # label_list.append(torch.tensor(gt_mask))\n",
    "            center_list.append(center_point)\n",
    "            bbox_list.append(np.array([x_min,y_min,x_max,y_max]))\n",
    "            \n",
    "        img=torch.tensor(img).float()\n",
    "        label_list=torch.tensor(label_list).long()\n",
    "        center_list=torch.tensor(center_list)\n",
    "        bbox_list=torch.tensor(bbox_list)\n",
    "\n",
    "        return (img,label_list,center_list,bbox_list)\n",
    "    \n",
    "train_lapa_set=LaPaDataset(data_path='./LaPa/train/images',label_path='./LaPa/train/labels')\n",
    "train_lapa_loader=DataLoader(train_lapa_set,batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class finetuneSAM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_encoder:ImageEncoderViT,\n",
    "            mask_decoder:MaskDecoder,\n",
    "            prompt_encoder:PromptEncoder,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder=image_encoder\n",
    "        self.mask_decoder=mask_decoder\n",
    "        self.prompt_encoder=prompt_encoder\n",
    "\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad=False\n",
    "        for param in self.prompt_encoder.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "    def forward(self,image:torch.Tensor,prompt:torch.Tensor,type:str):\n",
    "        with torch.no_grad():\n",
    "            image_embedding=self.image_encoder(image)\n",
    "            if type==\"single\":\n",
    "                label=torch.ones(size=prompt.shape[:-1],dtype=torch.long,device=prompt.device)\n",
    "                sparse_embeddings,dense_embeddings=self.prompt_encoder(\n",
    "                    points=(prompt,label),boxes=None,masks=None,\n",
    "                )\n",
    "            else:\n",
    "                sparse_embeddings,dense_embeddings=self.prompt_encoder(\n",
    "                    points=None,boxes=prompt,masks=None,\n",
    "                )\n",
    "        low_res_masks,_=self.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        ori_res_masks=F.interpolate(low_res_masks,size=(image.shape[2],image.shape[3]),mode='bilinear',\n",
    "                                      align_corners=False,)\n",
    "        return ori_res_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(dataloader,model,lr=1e-4,epochs=20,method='single'):\n",
    "\n",
    "#     model.train()\n",
    "#     seg_loss=monai.losses.DiceCELoss(sigmoid=True,squared_pred=True,reduction='mean')\n",
    "#     cross_loss=nn.BCEWithLogitsLoss(reduction='mean')\n",
    "#     optimizer=torch.optim.Adam(model.mask_decoder.parameters(),lr=lr,weight_decay=1e-4)\n",
    "#     # for para in model.image_decoder.parameters():\n",
    "#     #     para.required_grad=False\n",
    "    \n",
    "#     # for para in model.prompt_decoder.parameters():\n",
    "#     #     para.required_grad=False\n",
    "#     total_step=len(dataloader)\n",
    "#     for epoch in range(epochs):\n",
    "#         step=0\n",
    "#         for batch in dataloader:\n",
    "#             img,embedding_list,label_list,center_list,bbox_list=batch\n",
    "#             step+=1\n",
    "#             optimizer.zero_grad()\n",
    "#             print(embedding_list.shape)\n",
    "#             for i in range(10):\n",
    "#                 embedding=embedding_list[:,i,:]\n",
    "#                 labels=label_list[i]\n",
    "#                 center_points=center_list[i]\n",
    "                \n",
    "#                 box=bbox_list[:,i,:]\n",
    "#                 box=box[:,None,:]\n",
    "#                 center=center_list[:,i,:]\n",
    "#                 if method=='box':\n",
    "#                     sparse_embeddings,dense_embeddings=model.prompt_decoder(\n",
    "#                         points=None,\n",
    "#                         boxer=box,\n",
    "#                         masks=None\n",
    "#                     )\n",
    "#                 elif method=='single':\n",
    "#                     sparse_embeddings,dense_embeddings=model.prompt_decoder(\n",
    "#                         points=center,boxer=None,masks=None\n",
    "#                     )\n",
    "#                 mask_pred,_=model.mask_decoder(\n",
    "#                     image_embeddings=embedding,\n",
    "#                     image_pe=model.prompt_decoder.get_dense_pe(),\n",
    "#                     sparse_propt_embeddings=sparse_embeddings,\n",
    "#                     dense_prompt_embeddings=dense_embeddings,\n",
    "#                     multimask_output=False\n",
    "#                 )\n",
    "#                 loss1=seg_loss(seg,label)\n",
    "#                 loss2=cross_loss(seg,label)\n",
    "#                 loss=loss1+loss2\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 if int(100*i/step)%10==0:\n",
    "#                     print('epoch[{}/{}],step [{}/{}],loss:{:.4f}'.format(epoch+1,epochs,i+1,total_step,loss.cpu().item()))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,output_path='./res/finetune'\n",
    "          ,data_path='./LaPa/train/images',label_path='./LaPa/train/labels'):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    sam_model = model\n",
    "    finetune_model = finetuneSAM(\n",
    "        image_encoder=sam_model.image_encoder,\n",
    "        mask_decoder=sam_model.mask_decoder,\n",
    "        prompt_encoder=sam_model.prompt_encoder,\n",
    "    ).to(device)\n",
    "    finetune_model.train()\n",
    "\n",
    "    optimizer=torch.optim.Adam(finetune_model.mask_decoder.parameters(),lr=1e-4,weight_decay=0.01)\n",
    "    seg_loss=monai.losses.DiceLoss(sigmoid=True,squared_pred=True,reduction=\"mean\")\n",
    "    ce_loss=nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "    num_epochs=10\n",
    "    losses=[]\n",
    "    best_loss=1e10\n",
    "\n",
    "    train_dataset=LaPaDataset(data_path=data_path,label_path=label_path)\n",
    "    train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "    # val_dataset=LaPaDataset(data_path=)\n",
    "    # val_loader=DataLoader(val_dataset, batch_size=16,shuffle=False)\n",
    "\n",
    "    start_epoch=0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epoch_loss=0\n",
    "        for img,label_list,center_list,box_list in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i in range(0,10):\n",
    "                img=img.to(device)\n",
    "                box=box_list[:,i,:].to(device)\n",
    "                center_point=center_list[:,i,:].to(device)\n",
    "                label=label_list[:,i,:].to(device)\n",
    "                \n",
    "                finetune_pred=finetune_model(img,center_point,'single')\n",
    "                loss=seg_loss(finetune_pred,label)+ce_loss(finetune_pred,label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                finetune_pred=finetune_model(img,box,'box')\n",
    "                loss=seg_loss(finetune_pred,label)+ce_loss(finetune_pred,label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                \n",
    "\n",
    "        epoch_loss /= len(train_loader) * 3\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        print(\n",
    "            f'Time: {datetime.now().strftime(\"%Y%m%d-%H%M\")}, Epoch: {epoch}, Loss: {epoch_loss}'\n",
    "        )\n",
    "\n",
    "        checkpoint = {\n",
    "            'model': finetune_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': epoch_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, join(output_path, 'finetune_latest.pth'))\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            checkpoint = {\n",
    "                'model': finetune_model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'loss': epoch_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, join(output_path, 'finetune_best.pth'))\n",
    "\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Dice + Cross Entropy Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "        plt.savefig(join(output_path, f'finetune_loss.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msam_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, output_path, data_path, label_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs):\n\u001b[0;32m     28\u001b[0m     epoch_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img,label_list,center_list,box_list \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     30\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[1;32md:\\anoconda\\envs\\sam\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anoconda\\envs\\sam\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\anoconda\\envs\\sam\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\anoconda\\envs\\sam\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[60], line 36\u001b[0m, in \u001b[0;36mLaPaDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     31\u001b[0m gt_mask\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8(label\u001b[38;5;241m==\u001b[39mi)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# if (gt_mask.any()==i)==False:\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#     center_list.append(None)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#     bbox_list.append(None)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#     continue\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m y_idx,x_idx\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mgt_mask\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m)\n\u001b[0;32m     37\u001b[0m y_min,y_max\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmin(y_idx),np\u001b[38;5;241m.\u001b[39mmax(y_idx)\n\u001b[0;32m     38\u001b[0m x_min,x_max\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmin(x_idx),np\u001b[38;5;241m.\u001b[39mmax(x_idx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(sam_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
